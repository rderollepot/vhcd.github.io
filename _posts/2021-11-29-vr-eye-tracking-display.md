---
published: false
title: Live VR eye-tracking data visualization
---
Eye-tracking is widely used to study driver's behavior in simulated environment. However, we're used to either have glasses (e.g. [Pupil Core](https://pupil-labs.com/products/core/)) or fixed setups (e.g [Smart Eye Pro](https://smarteye.se/research-instruments/se-pro/)); and the newly released VR headsets with included eye-tracking (e.g. [Vive Pro Eye](https://www.vive.com/eu/product/vive-pro-eye/overview/) bring new challenges. The one we'll cover today is the visualization of live eye-tracking data, why and how we managed to implement it in our platform.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;ve managed to display live VR <a href="https://twitter.com/hashtag/EyeTracking?src=hash&amp;ref_src=twsrc%5Etfw">#EyeTracking</a> (Vive Pro Eye) data on a <a href="https://twitter.com/hashtag/UE4?src=hash&amp;ref_src=twsrc%5Etfw">#UE4</a> spectator screen, which is recorded alongside raw data with <a href="https://twitter.com/hashtag/LabStreamingLayer?src=hash&amp;ref_src=twsrc%5Etfw">#LabStreamingLayer</a>. I&#39;ll probably write up a blog entry about this, as it&#39;s not trivial but really useful to have. <a href="https://t.co/hgNsn3cpgk">pic.twitter.com/hgNsn3cpgk</a></p>&mdash; Bertrand Richard (@brifsttar) <a href="https://twitter.com/brifsttar/status/1460649252485570561?ref_src=twsrc%5Etfw">November 16, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

# Eye-tracking data

# Why live visualization?

# Implementation

## Spectator screen

## Drawing
